{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    A perceptron is one linear layer \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): size of the input features\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the perceptron\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "                x_in.shape should be (batch, num_features)\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "y = torch.sigmoid(x)\n",
    "plt.plot(x.numpy(), y.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "y = torch.tanh(x)\n",
    "\n",
    "plt.plot(x.numpy(), y.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x.numpy(), y.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prelu = nn.PReLU(num_parameters=1)\n",
    "x = torch.arange(-5., 5., 0.1)\n",
    "y = prelu(x)\n",
    "\n",
    "plt.plot(x.numpy(), y.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "x_input = torch.randn(1, 3)\n",
    "y_output = softmax(x_input)\n",
    "print(x_input)\n",
    "print(y_output)\n",
    "print(torch.sum(y_output, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.randn(3, 5)\n",
    "loss = mse_loss(outputs, targets)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "outputs = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.tensor([1, 0, 3], dtype=torch.int64)\n",
    "loss = ce_loss(outputs, targets)\n",
    "loss.backward()\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
    "print(probabilities)\n",
    "\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32).view(4, 1)\n",
    "loss = bce_loss(probabilities, targets)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "nlpbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
