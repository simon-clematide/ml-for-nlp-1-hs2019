{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_CENTER = (3, 3)\n",
    "RIGHT_CENTER = (3, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \"\"\" A Perceptron is one Linear layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): size of the input features\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the MLP\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, 1)\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_data(batch_size, left_center=LEFT_CENTER, right_center=RIGHT_CENTER):\n",
    "    x_data = []\n",
    "    y_targets = np.zeros(batch_size)\n",
    "    for batch_i in range(batch_size):\n",
    "        if np.random.random() > 0.5:\n",
    "            x_data.append(np.random.normal(loc=left_center))\n",
    "        else:\n",
    "            x_data.append(np.random.normal(loc=right_center))\n",
    "            y_targets[batch_i] = 1\n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(perceptron, x_data, y_truth, n_samples=1000, ax=None, epoch=None, \n",
    "                      title='', levels=[0.3, 0.4, 0.5], linestyles=['--', '-', '--']):\n",
    "    y_pred = perceptron(x_data)\n",
    "    y_pred = (y_pred > 0.5).long().data.numpy().astype(np.int32)\n",
    "\n",
    "    x_data = x_data.data.numpy()\n",
    "    y_truth = y_truth.data.numpy().astype(np.int32)\n",
    "\n",
    "    n_classes = 2\n",
    "\n",
    "    all_x = [[] for _ in range(n_classes)]\n",
    "    all_colors = [[] for _ in range(n_classes)]\n",
    "    \n",
    "    colors = ['black', 'white']\n",
    "    markers = ['o', '*']\n",
    "    \n",
    "    for x_i, y_pred_i, y_true_i in zip(x_data, y_pred, y_truth):\n",
    "        all_x[y_true_i].append(x_i)\n",
    "        if y_pred_i == y_true_i:\n",
    "            all_colors[y_true_i].append(\"white\")\n",
    "        else:\n",
    "            all_colors[y_true_i].append(\"black\")\n",
    "        #all_colors[y_true_i].append(colors[y_pred_i])\n",
    "\n",
    "    all_x = [np.stack(x_list) for x_list in all_x]\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "        \n",
    "    for x_list, color_list, marker in zip(all_x, all_colors, markers):\n",
    "        ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor=\"black\", marker=marker, facecolor=color_list, s=300)\n",
    "    \n",
    "        \n",
    "    xlim = (min([x_list[:,0].min() for x_list in all_x]), \n",
    "            max([x_list[:,0].max() for x_list in all_x]))\n",
    "            \n",
    "    ylim = (min([x_list[:,1].min() for x_list in all_x]), \n",
    "            max([x_list[:,1].max() for x_list in all_x]))\n",
    "            \n",
    "    # hyperplane\n",
    "    \n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    \n",
    "    Z = perceptron(torch.tensor(xy, dtype=torch.float32)).detach().numpy().reshape(XX.shape)\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=levels, linestyles=linestyles)    \n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    if epoch is not None:\n",
    "        plt.text(xlim[0], ylim[1], \"Epoch = {}\".format(str(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "x_data, y_truth = get_toy_data(batch_size=1000)\n",
    "\n",
    "x_data = x_data.data.numpy()\n",
    "y_truth = y_truth.data.numpy()\n",
    "\n",
    "left_x = []\n",
    "right_x = []\n",
    "left_colors = []\n",
    "right_colors =  []\n",
    "\n",
    "for x_i, y_true_i in zip(x_data, y_truth):\n",
    "    color = 'black'\n",
    "\n",
    "    if y_true_i == 0:\n",
    "        left_x.append(x_i)\n",
    "        left_colors.append(color)\n",
    "\n",
    "    else:\n",
    "        right_x.append(x_i)\n",
    "        right_colors.append(color)\n",
    "\n",
    "left_x = np.stack(left_x)\n",
    "right_x = np.stack(right_x)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(10,4))\n",
    "\n",
    "ax.scatter(left_x[:, 0], left_x[:, 1], color=left_colors, marker='*', s=100)\n",
    "ax.scatter(right_x[:, 0], right_x[:, 1], facecolor='white', edgecolor=right_colors, marker='o', s=100)\n",
    "\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training + intermittent data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "input_dim = 2\n",
    "\n",
    "batch_size = 1000\n",
    "n_epochs = 12\n",
    "n_batches = 5\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "perceptron = Perceptron(input_dim=input_dim)\n",
    "optimizer = optim.Adam(params=perceptron.parameters(), lr=lr)\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "x_data_static, y_truth_static = get_toy_data(batch_size)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "visualize_results(perceptron, x_data_static, y_truth_static, ax=ax, title='Initial Model State')\n",
    "plt.axis('off')\n",
    "#plt.savefig('initial.png')\n",
    "\n",
    "change = 1.0\n",
    "last = 10.0\n",
    "epsilon = 1e-3\n",
    "epoch = 0\n",
    "while change > epsilon or epoch < n_epochs or last > 0.3:\n",
    "#for epoch in range(n_epochs):\n",
    "    for _ in range(n_batches):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_data, y_target = get_toy_data(batch_size)\n",
    "        y_pred = perceptron(x_data).squeeze()\n",
    "        loss = bce_loss(y_pred, y_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        change = abs(last - loss_value)\n",
    "        last = loss_value\n",
    "               \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "    visualize_results(perceptron, x_data_static, y_truth_static, ax=ax, epoch=epoch, \n",
    "                      title=f\"{loss_value}; {change}\")\n",
    "    plt.axis('off')\n",
    "    epoch += 1\n",
    "    #plt.savefig('epoch{}_toylearning.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "\n",
    "axes[0].scatter(left_x[:, 0], left_x[:, 1], facecolor='white',edgecolor='black', marker='o', s=300)\n",
    "axes[0].scatter(right_x[:, 0], right_x[:, 1], facecolor='white', edgecolor='black', marker='*', s=300)\n",
    "axes[0].axis('off');\n",
    "visualize_results(perceptron, x_data_static, y_truth_static, epoch=None, levels=[0.5], ax=axes[1])\n",
    "axes[1].axis('off');\n",
    "plt.savefig('perceptron_final.png')\n",
    "plt.savefig('perceptron_final.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "nlpbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
